# Проект по Генерации Текста с использованием LSTM

## Описание Проекта
Этот проект направлен на создание и обучение модели генерации текста на основе архитектуры LSTM с использованием библиотеки TensorFlow. Модель обучается на текстовом корпусе, который содержится в файле prob.txt, и затем генерирует новые текстовые последовательности на основе заданного начального текста (seed_text).

## Установка Зависимостей
* Перед запуском проекта необходимо установить все необходимые зависимости. Для этого используйте следующую команду:
```
pip install tensorflow nltk langdetect matplotlib scikit-learn joblib
```
## Загрузка Модели Языка
* Для определения языка текста используется библиотека langdetect. Убедитесь, что она установлена и работает корректно.

## Запуск Проекта
1.  Запуск обучения модели :
Запустите скрипт webui.py для обучения модели и генерации текста.
```
python webui.py
```
2.  Использование TensorBoard :
Для визуализации метрик обучения используйте TensorBoard.
```
tensorboard --logdir=./training/logs
```
## Параметры Модели
*   Embedding Dimension : 128**
*   LSTM Units : 128
*   Dropout Rate : 0.2
*   Recurrent Dropout Rate : 0.2
*   Batch Size : 64
*   Эпохи : 100
*   Beam Width : 3
*   Максимальная Длина Последовательности : 50
*   Temperature : 0.7
  ##  функции
*   Предварительная Обработка Текста
*   Функция preprocess_text выполняет следующие шаги:
1.  Определение языка текста с использованием langdetect.
2.  Приведение текста к нижнему регистру.
3.  Замена специальных символов на стандартизированные.
4.  Удаление нежелательных символов, оставляя основные знаки препинания.
5.  Учитывание многоточий и сокращений.
6.  Замена множественных пробелов на одинарные.
7.  Удаление пробелов перед знаками препинания.
8.  Создание Последовательностей
9.  Функция create_sequences создаёт последовательности из текстовых данных с использованием библиотеки Tokenizer и pad_sequences.

##  Построение Модели
*   Функция build_model строит модель с использованием слоёв Embedding, Bidirectional LSTM, LayerNormalization, Dropout и Dense.

##  Генерация Текста
*   Функция beam_search_generate_text генерирует текст с использованием механизма beam search, который учитывает несколько лучших вариантов на каждом шаге.

##  Learning Rate Scheduler
*   Функция lr_scheduler уменьшает скорость обучения по экспоненте.

##  Построение Графиков Метрик
*   Функция plot_metrics строит и сохраняет графики метрик обучения и валидации.

##  Логирование
*   Логирование выполняется в файл training.log и включает информацию о различных этапах выполнения, таких как предобработка текста, создание последовательностей, обучение модели и генерация текста.

##  Пример Использования
*   Обработка Текста :
*   Файл prob.txt содержит исходный текстовый корпус.
*   Пример содержимого prob.txt:
```
И от лености или со скуки он он он я я не пути готовых к лесу камень камень ты это это не кисти прозрачный стрекоза во работы летом снегу глаза глаза смотри предсмертная это кукушка остался сквозь и стало всё к цветка прохладный вся воде ничем сосновая лист ночь это мои лишь как ивы задумалась чтоб
```
##  Запуск Обучения :
*   Запустите скрипт webui.py для обучения модели и генерации текста.
```
python webui.py
```
##  Просмотр Метрик :
*   Используйте TensorBoard для просмотра метрик обучения.
```
tensorboard --logdir=./training/logs
```
##  Генерация Текста :
*   После обучения модели генерируется текст на основе заданного начального текста (seed_text).
```
И от лености или со скуки он он он я я не пути готовых к лесу камень камень ты это это не кисти прозрачный стрекоза во работы летом снегу глаза глаза смотри предсмертная это кукушка остался сквозь и стало всё к цветка прохладный вся воде ничем сосновая лист ночь это мои лишь как ивы задумалась чтоб
```
